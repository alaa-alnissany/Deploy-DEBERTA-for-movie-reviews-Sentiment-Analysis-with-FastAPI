{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Movie Review Sentiment Analysis using LSTM"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "id": "DOtFxlRf8eja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Steps:\n",
        "> * #### Importing important depenedencies\n",
        "> * #### EDA\n",
        "> * #### Text Preprocessing\n",
        "> * #### Model Building\n",
        "> * #### Evaluation\n",
        "> * #### Summary\n",
        "---"
      ],
      "metadata": {
        "id": "eeC8JdT78ejg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing important dependencies"
      ],
      "metadata": {
        "id": "3V6NLn0K8ejh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('max_colwidth',400)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datasets\n",
        "%matplotlib inline\n",
        "from bs4 import BeautifulSoup\n",
        "import re,string,unicodedata\n",
        "\n",
        "# ML libraries (Preprocessing, models..)\n",
        "import lightgbm as lgb\n",
        "from scipy import stats\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "\n",
        "# NLTK \n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# CNN, LSTM and Embedding \n",
        "from keras import backend as K\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification , AutoTokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, ZeroPadding1D, Add, Flatten, Dot, Concatenate, Lambda\n",
        "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D, TimeDistributed, Attention\n",
        "from keras.layers import InputSpec, Layer\n",
        "from keras.models import Model, load_model\n",
        "from keras.optimizers import Adam,Adagrad\n",
        "from tensorflow_addons.optimizers import AdamW\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping"
      ],
      "metadata": {
        "trusted": true,
        "id": "e8pogqV98eji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text mining and EDA"
      ],
      "metadata": {
        "id": "UzDNvb9H8ejk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting a subset of data to be faster in demonstration\n",
        "train_df = pd.read_csv('/kaggle/input/shai-training-2023-a-level-2/Train.csv',encoding=\"utf-8\")\n",
        "valid_df = pd.read_csv('/kaggle/input/shai-training-2023-a-level-2/Valid.csv',encoding=\"utf-8\")\n",
        "test_df = pd.read_csv('/kaggle/input/shai-training-2023-a-level-2/Test.csv',encoding=\"utf-8\")\n",
        "sub = pd.read_csv('/kaggle/input/shai-training-2023-a-level-2/sample_submission.csv')\n",
        "print('Train: '+ str(len(train_df)))\n",
        "print('Valid: '+ str(len(valid_df)))\n",
        "print('Test: '+ str(len(test_df)))\n",
        "train_df[\"label\"] = train_df.label.astype(float)\n",
        "train_df.head(10)"
      ],
      "metadata": {
        "trusted": true,
        "id": "pWXk85kn8ejl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The distribution of sentiments\n",
        "train_df.groupby('label').count().plot(kind='bar')"
      ],
      "metadata": {
        "trusted": true,
        "id": "IP-eeX6q8ejm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This means that the no. of positive reviews is equal to the no. of negative reviews in the dataset. This is a good thing since it means our dataset is not skewed."
      ],
      "metadata": {
        "id": "Rd1ywY9c8ejm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate review lengths\n",
        "review_len = pd.Series([len(review.split()) for review in train_df['text']])\n",
        "\n",
        "# The distribution of review text lengths\n",
        "review_len.plot(kind='box')"
      ],
      "metadata": {
        "trusted": true,
        "id": "CV0CxNuU8ejn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let us visualize how long our sentences are in the training data"
      ],
      "metadata": {
        "id": "VPDWAUxd8ejo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_theme(\n",
        "    context='notebook',\n",
        "    style='darkgrid',\n",
        "    palette='deep',\n",
        "    font='sans-serif',\n",
        "    font_scale=1,\n",
        "    color_codes=True,\n",
        "    rc=None,\n",
        ")\n",
        "\n",
        "plt.figure(figsize = (10,12))\n",
        "sns.histplot(review_len)"
      ],
      "metadata": {
        "trusted": true,
        "id": "MZdt3ElY8ejo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(14,7))\n",
        "train_df['length'] = train_df.text.str.split().apply(len)\n",
        "ax1 = fig.add_subplot(122)\n",
        "sns.histplot(train_df[train_df['label']==1]['length'], ax=ax1,color='green')\n",
        "describe = train_df.length[train_df.label==1].describe().to_frame().round(2)\n",
        "\n",
        "ax2 = fig.add_subplot(121)\n",
        "ax2.axis('off')\n",
        "font_size = 14\n",
        "bbox = [0, 0, 1, 1]\n",
        "table = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\n",
        "table.set_fontsize(font_size)\n",
        "fig.suptitle('Distribution of text length for positive sentiment reviews.', fontsize=16)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "EL43dBSL8ejp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(14,7))\n",
        "ax1 = fig.add_subplot(122)\n",
        "sns.histplot(train_df[train_df['label']==0]['length'], ax=ax1,color='red')\n",
        "describe = train_df.length[train_df.label==0].describe().to_frame().round(2)\n",
        "\n",
        "ax2 = fig.add_subplot(121)\n",
        "ax2.axis('off')\n",
        "font_size = 14\n",
        "bbox = [0, 0, 1, 1]\n",
        "table = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\n",
        "table.set_fontsize(font_size)\n",
        "fig.suptitle('Distribution of text length for Negative sentiment reviews.', fontsize=16)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "mTSaKZMZ8ejp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WORDCLOUD FOR NEGATIVE TEXT (LABEL - 0)"
      ],
      "metadata": {
        "id": "XOh-nDC98ejq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (20,20)) # Negative Review Text\n",
        "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(train_df[train_df.label == 0].text))\n",
        "plt.imshow(wc , interpolation = 'bilinear')"
      ],
      "metadata": {
        "trusted": true,
        "id": "pLzZ4Fti8ejq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WORDCLOUD FOR POSITIVE TEXT (LABEL - 1)\n"
      ],
      "metadata": {
        "id": "5G38Tnix8ejr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (20,20)) # Positive Review Text\n",
        "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(train_df[train_df.label == 1].text))\n",
        "plt.imshow(wc , interpolation = 'bilinear')"
      ],
      "metadata": {
        "trusted": true,
        "id": "1a4Bh5bm8ejr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Text Preprocessing\n",
        "\n",
        "Text preprocessing is an essential step in natural language processing (NLP) and machine learning projects, including sentiment analysis. Here are some of the typical text preprocessing steps for a sentiment analysis NLP project:\n",
        "\n",
        "* `Text Cleaning`: Remove unnecessary characters, such as punctuation, special characters, numbers, and stop words (common words that don't carry much meaning, such as \"the,\" \"a,\" \"and,\" etc.)\n",
        "\n",
        "* `Lowercasing`: Convert all the words to lowercase to treat different cases of the same word as the same (e.g., \"good\" and \"Good\").\n",
        "\n",
        "* `Tokenization`: Split the text into individual words or phrases (tokens).\n",
        "\n",
        "* `Stemming/Lemmatization`: Reduce words to their base or root form to normalize the text. Stemming removes the suffixes from words, while lemmatization reduces words to their base form based on their part of speech.\n",
        "\n",
        "* `Parts of Speech Tagging`: Identify the part of speech of each word (noun, verb, adjective, etc.) to help determine the meaning and context of the sentence.\n",
        "\n",
        "* `Sentiment lexicon-based feature extraction`: Assign a sentiment score to each token based on a sentiment lexicon (a dictionary of words and their sentiment polarity).\n",
        "\n",
        "* `Feature Encoding`: Convert text data into numerical representations that machine learning algorithms can understand, such as one-hot encoding, TF-IDF, or word embedding.\n",
        "\n",
        "* `Feature Selection`: Select the most relevant features to reduce the dimensionality of the data and improve the model's performance."
      ],
      "metadata": {
        "id": "XK4CU92_8ejr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turning all text to lowercase\n",
        "train_df['text'] = train_df['text'].str.lower()\n",
        "valid_df['text'] = valid_df['text'].str.lower()\n",
        "test_df['text'] = test_df['text'].str.lower()\n",
        "train_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "amU0KK768ejs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing punctuation\n",
        "exclude = set(string.punctuation) \n",
        "\n",
        "def remove_punctuation(x): \n",
        "    try: \n",
        "        x = ''.join(ch for ch in x if ch not in exclude) \n",
        "    except: \n",
        "        pass \n",
        "    return x \n",
        "\n",
        "train_df['text'] = train_df['text'].apply(remove_punctuation)\n",
        "valid_df['text'] = valid_df['text'].apply(remove_punctuation)\n",
        "test_df['text'] = test_df['text'].apply(remove_punctuation)\n",
        "train_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "7WK3iHzE8ejs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing the html strips\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "# Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "# Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "# Apply function on review column\n",
        "train_df['text'] = train_df['text'].apply(denoise_text)\n",
        "valid_df['text'] = valid_df['text'].apply(denoise_text)\n",
        "test_df['text'] = test_df['text'].apply(denoise_text)\n",
        "train_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "qegCHdX18ejt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set stopwords to english\n",
        "stop=set(stopwords.words('english'))\n",
        "stopword_list=nltk.corpus.stopwords.words('english')\n",
        "print(stop)\n",
        "\n",
        "# Create an instance of the TweetTokenizer class\n",
        "# Tokenization of text\n",
        "tokenizer=TweetTokenizer()\n",
        "\n",
        "\n",
        "# removing the stopwords\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "#Apply function on review column\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "train_df['text'] = train_df['text'].apply(remove_stopwords)\n",
        "valid_df['text'] = valid_df['text'].apply(remove_stopwords)\n",
        "test_df['text'] = test_df['text'].apply(remove_stopwords)\n",
        "train_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "9ZghVXuW8eju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classical Models with TF-IDF, SVM, OneVsRest Classifer"
      ],
      "metadata": {
        "id": "nCnDguDn8eju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the TfidfVectorizer class with n-grams of size 1 and 2, \n",
        "# and use the TweetTokenizer to tokenize the text\n",
        "vectorizer = TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1, 2), tokenizer=tokenizer.tokenize)\n",
        "# Combine the text from the train and test dataframes into a list\n",
        "full_text = list(train_df['text'].values) + list(test_df['text'].values)\n",
        "# Fit the vectorizer on the combined text\n",
        "vectorizer.fit(full_text)\n",
        "# Use the fitted vectorizer to transform the text in the train, test and validation dataframe into a sparse matrix of TF-IDF values\n",
        "train_vectorized = vectorizer.transform(train_df['text'])\n",
        "test_vectorized = vectorizer.transform(test_df['text'])\n",
        "valid_vectors = vectorizer.transform(valid_df['text'])"
      ],
      "metadata": {
        "trusted": true,
        "id": "GMZNoPGA8ejv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = train_df['label']"
      ],
      "metadata": {
        "trusted": true,
        "id": "dqqDN4Yu8ejv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using OneVsRestClassifier and Logistic Regression"
      ],
      "metadata": {
        "id": "a8jQDPzV8ejv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logreg = LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n",
        "ovr = OneVsRestClassifier(logreg)"
      ],
      "metadata": {
        "trusted": true,
        "id": "_5nWNsaD8ejv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "ovr.fit(train_vectorized, y)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Q1z2a7VW8ejw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = cross_val_score(ovr, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\n",
        "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
      ],
      "metadata": {
        "trusted": true,
        "id": "X0jv54Hd8ejw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using SVM Model"
      ],
      "metadata": {
        "id": "kADfVBDd8ejw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "svc = LinearSVC(dual=False)\n",
        "scores = cross_val_score(svc, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\n",
        "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
      ],
      "metadata": {
        "trusted": true,
        "id": "HwXqQxXa8ejx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ovr.fit(train_vectorized, y);\n",
        "svc.fit(train_vectorized, y);"
      ],
      "metadata": {
        "trusted": true,
        "id": "zkrPtSdL8ejx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Changing the classifier by a LSTM and EMBEDDING model"
      ],
      "metadata": {
        "id": "IdR8O-Tc8ejx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the Tokenizer class with options to lowercase the text and remove all filters\n",
        "tk = Tokenizer(lower = True, filters='')\n",
        "tk.fit_on_texts(full_text)"
      ],
      "metadata": {
        "trusted": true,
        "id": "pDz6tihp8ejx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the fitted tokenizer to convert the text in the train dataframe into a sequence of integer indexes\n",
        "train_tokenized = tk.texts_to_sequences(train_df['text'])\n",
        "# Use the fitted tokenizer to convert the text in the test dataframe into a sequence of integer indexes\n",
        "test_tokenized = tk.texts_to_sequences(test_df['text'])"
      ],
      "metadata": {
        "trusted": true,
        "id": "QJoc96m08ejy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 100\n",
        "X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
        "X_test = pad_sequences(test_tokenized, maxlen = max_len)"
      ],
      "metadata": {
        "trusted": true,
        "id": "9vzvmUL38ejy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used the `texts_to_sequences` method of the fitted tokenizer to convert the text in the `train_df['text']` and `test_df['text']` columns into sequences of `integer indexes`. Each unique word in the text is assigned a unique integer index based on its frequency in the text. The resulting sequences are then stored in the `train_tokenized` and `test_tokenized` variables, respectively. These integer sequences can be used as input to machine learning algorithms that require numerical input."
      ],
      "metadata": {
        "id": "6ah0leha8ejy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the pre-trained word embedding file\n",
        "embedding_path = \"/kaggle/input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\n",
        "# Set the size of the word embeddings to 300\n",
        "embed_size = 300\n",
        "# Set the maximum number of features to 30,000\n",
        "max_features = 30000"
      ],
      "metadata": {
        "trusted": true,
        "id": "T2URa8nU8ejy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have set up the parameters for the pre-trained word embeddings that will be used to initialize the embedding layer in the neural network. The `embedding_path` variable specifies the path to the file containing the pre-trained word embeddings. The `embed_size` variable specifies the size of the word embeddings, which is set to 300. The `max_features` variable specifies the maximum number of features (i.e. words) that will be included in the vocabulary, which is set to `30,000`. These parameters will be used later when defining the embedding layer in the neural network."
      ],
      "metadata": {
        "id": "GjfHyoQx8ejz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function returns a tuple with the word and its corresponding coefficients as a numpy array\n",
        "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "# Load the pre-trained embeddings file and create a dictionary of word vectors\n",
        "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n",
        "# Get the index of each word in the tokenizer\n",
        "word_index = tk.word_index\n",
        "# Set the number of words to be used as the minimum between the maximum features allowed and the number of words in the tokenizer\n",
        "nb_words = min(max_features, len(word_index))\n",
        "# Initialize an embedding matrix of zeros with shape (nb_words + 1, embed_size)\n",
        "embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
        "# Iterate over each word in the tokenizer and its index\n",
        "for word, i in word_index.items():\n",
        "    # If the index of the word is greater than or equal to the maximum features allowed, skip it\n",
        "    if i >= max_features: continue\n",
        "    # Get the embedding vector of the word from the pre-trained embeddings dictionary\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    # If the embedding vector is not None, add it to the embedding matrix at the index of the word in the tokenizer\n",
        "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "trusted": true,
        "id": "Pxi3Cgq-8ej-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using OneHotEncoder"
      ],
      "metadata": {
        "id": "K6dNqm7l8ej_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ohe = OneHotEncoder(sparse=False)\n",
        "y_ohe = ohe.fit_transform(y.values.reshape(-1, 1))"
      ],
      "metadata": {
        "trusted": true,
        "id": "zFnzp-B38ej_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1: GRU + CONV + LSTM"
      ],
      "metadata": {
        "id": "EzMTNOav8ej_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the model\n",
        "def build_model1(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
        "    file_path = \"best_model_1.h5\"\n",
        "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
        "                                  save_best_only = True, mode = \"min\")\n",
        "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
        "    \n",
        "    inp = Input(shape = (max_len,))\n",
        "    x = Embedding(30001, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
        "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
        "\n",
        "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
        "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
        "    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n",
        "    max_pool1_gru = GlobalMaxPooling1D()(x1)\n",
        "    \n",
        "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
        "    avg_pool3_gru = GlobalAveragePooling1D()(x3)\n",
        "    max_pool3_gru = GlobalMaxPooling1D()(x3)\n",
        "    \n",
        "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
        "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
        "    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n",
        "    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n",
        "    \n",
        "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
        "    avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n",
        "    max_pool3_lstm = GlobalMaxPooling1D()(x3)\n",
        "    \n",
        "    #x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
        "    #x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
        "    #avg_pool2_lstm = GlobalAveragePooling1D()(x1)\n",
        "    #max_pool2_lstm = GlobalMaxPooling1D()(x1)\n",
        "    \n",
        "    #x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
        "    #avg_pool4_lstm = GlobalAveragePooling1D()(x3)\n",
        "    #max_pool4_lstm = GlobalMaxPooling1D()(x3)\n",
        "    \n",
        "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n",
        "                    avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n",
        "                    #avg_pool2_lstm, max_pool2_lstm, avg_pool4_lstm, max_pool4_lstm])\n",
        "    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
        "    x = Dense(2, activation = \"sigmoid\")(x)\n",
        "    model = Model(inputs = inp, outputs = x)\n",
        "    model.compile(loss = \"binary_crossentropy\",\n",
        "                  optimizer = AdamW(lr = lr, weight_decay = lr_d),\n",
        "                  metrics = [\"accuracy\"])\n",
        "    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 30, validation_split=0.1,\n",
        "                        verbose = 1, callbacks = [check_point, early_stop])\n",
        "    model = load_model(file_path)\n",
        "    return model"
      ],
      "metadata": {
        "trusted": true,
        "id": "DxjLlR7U8ekA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = build_model1(lr = 1e-3,\n",
        "                      lr_d = 1e-10,\n",
        "                      units = 64,\n",
        "                      spatial_dr = 0.3,\n",
        "                      kernel_size1=3,\n",
        "                      kernel_size2=2,\n",
        "                      dense_units=32,\n",
        "                      dr=0.2,\n",
        "                      conv_size=64)"
      ],
      "metadata": {
        "trusted": true,
        "id": "5i3IJroU8ekA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = build_model1(lr = 1e-3,\n",
        "                      lr_d = 1e-10,\n",
        "                      units = 128,\n",
        "                      spatial_dr = 0.5,\n",
        "                      kernel_size1=3,\n",
        "                      kernel_size2=2,\n",
        "                      dense_units=64,\n",
        "                      dr=0.3,\n",
        "                      conv_size=64)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Bqyo6stA8ekA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2: GRU + CONV + LSTM + ATTENTION"
      ],
      "metadata": {
        "id": "mwaz_o_m8ekB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model2(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
        "    file_path = \"best_model_2.h5\"\n",
        "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
        "                                  save_best_only = True, mode = \"min\")\n",
        "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
        "\n",
        "    inp = Input(shape = (max_len,))\n",
        "    x = Embedding(30001, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
        "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
        "\n",
        "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
        "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
        "    \n",
        "    x_conv1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
        "    avg_pool1_gru = GlobalAveragePooling1D()(x_conv1)\n",
        "    max_pool1_gru = GlobalMaxPooling1D()(x_conv1)\n",
        "    \n",
        "    x_conv2 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
        "    avg_pool2_gru = GlobalAveragePooling1D()(x_conv2)\n",
        "    max_pool2_gru = GlobalMaxPooling1D()(x_conv2)\n",
        "    \n",
        "    \n",
        "    x_conv3 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
        "    avg_pool1_lstm = GlobalAveragePooling1D()(x_conv3)\n",
        "    max_pool1_lstm = GlobalMaxPooling1D()(x_conv3)\n",
        "    \n",
        "    x_conv4 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
        "    avg_pool2_lstm = GlobalAveragePooling1D()(x_conv4)\n",
        "    max_pool2_lstm = GlobalMaxPooling1D()(x_conv4)\n",
        "    \n",
        "    x_conv5 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
        "    avg_pool3_lstm = GlobalAveragePooling1D()(x_conv5)\n",
        "    max_pool3_lstm = GlobalMaxPooling1D()(x_conv5)\n",
        "    \n",
        "    # Attention Mechanism\n",
        "    attention_gru = Attention(max_len)([x_gru, x_gru])\n",
        "    attention_lstm = Attention(max_len)([x_lstm, x_lstm])\n",
        "    \n",
        "    # Flatten layers for attention layers\n",
        "    attention_gru = Flatten()(attention_gru)\n",
        "    attention_lstm = Flatten()(attention_lstm)\n",
        "    \n",
        "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool2_gru, max_pool2_gru,\n",
        "                    avg_pool1_lstm, max_pool1_lstm, avg_pool2_lstm, max_pool2_lstm,\n",
        "                    avg_pool2_lstm, max_pool3_lstm, attention_gru, attention_lstm])\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
        "    x = Dense(2, activation = \"sigmoid\")(x)\n",
        "    model = Model(inputs = inp, outputs = x)\n",
        "    model.compile(loss = \"binary_crossentropy\",\n",
        "                  optimizer = AdamW(lr = lr, weight_decay = lr_d),\n",
        "                  metrics = [\"accuracy\"])\n",
        "    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n",
        "                        verbose = 1, callbacks = [check_point, early_stop])\n",
        "    model = load_model(file_path)\n",
        "    return model"
      ],
      "metadata": {
        "trusted": true,
        "id": "BO6EOzQH8ekB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = build_model2(lr = 1e-4,\n",
        "                      lr_d = 1e-7,\n",
        "                      units = 64,\n",
        "                      spatial_dr = 0.5,\n",
        "                      kernel_size1=4,\n",
        "                      kernel_size2=3,\n",
        "                      dense_units=32,\n",
        "                      dr=0.2,\n",
        "                      conv_size=64)"
      ],
      "metadata": {
        "trusted": true,
        "id": "0ZhjR_yy8ekB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model4 = build_model2(lr = 1e-3,\n",
        "                      lr_d = 1e-5,\n",
        "                      units = 64,\n",
        "                      spatial_dr = 0.5,\n",
        "                      kernel_size1=3,\n",
        "                      kernel_size2=3,\n",
        "                      dense_units=64,\n",
        "                      dr=0.3,\n",
        "                      conv_size=64)"
      ],
      "metadata": {
        "trusted": true,
        "id": "i_AP_Dsm8ekC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model5 = build_model2(lr = 1e-3,\n",
        "                      lr_d = 1e-7,\n",
        "                      units = 64,\n",
        "                      spatial_dr = 0.3,\n",
        "                      kernel_size1=3,\n",
        "                      kernel_size2=3,\n",
        "                      dense_units=64,\n",
        "                      dr=0.4,\n",
        "                      conv_size=64)"
      ],
      "metadata": {
        "trusted": true,
        "id": "rU-YGhKu8ekC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 3: GRU + LSTM + ATTENTION"
      ],
      "metadata": {
        "id": "kUc1TtAc8ekC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model3(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
        "    file_path = \"best_model_3.h5\"\n",
        "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
        "                                  save_best_only = True, mode = \"min\")\n",
        "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
        "    \n",
        "    inp = Input(shape = (max_len,))\n",
        "    x = Embedding(30001, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
        "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
        "\n",
        "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
        "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
        "    \n",
        "    # Pad the shorter tensor with zeros along the time axis\n",
        "    x_gru = ZeroPadding1D(padding=(2, 0))(x_gru)\n",
        "    x_lstm = ZeroPadding1D(padding=(0, 2))(x_lstm)\n",
        "    \n",
        "    e1 = TimeDistributed(Dense(units*2, activation='tanh'))(x_gru)\n",
        "    e2 = TimeDistributed(Dense(units*2, activation='tanh'))(x_lstm)\n",
        "    e = Concatenate()([e1, e2])\n",
        "    score = Dense(1)(e)\n",
        "    attention_weights = Activation('softmax')(score)\n",
        "    context_vector = Dot(axes=1)([attention_weights, x_gru])\n",
        "    x = concatenate([context_vector, x_lstm], axis=1)\n",
        "    \n",
        "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x)\n",
        "    avg_pool1 = GlobalAveragePooling1D()(x1)\n",
        "    max_pool1 = GlobalMaxPooling1D()(x1)\n",
        "    \n",
        "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x)\n",
        "    avg_pool3 = GlobalAveragePooling1D()(x3)\n",
        "    max_pool3 = GlobalMaxPooling1D()(x3)\n",
        "    \n",
        "    x = concatenate([avg_pool1, max_pool1, avg_pool3, max_pool3])\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
        "    x = Dense(2, activation = \"sigmoid\")(x)\n",
        "    model = Model(inputs = inp, outputs = x)\n",
        "    model.compile(loss = \"binary_crossentropy\",\n",
        "                  optimizer = AdamW(lr = lr, weight_decay = lr_d),\n",
        "                  metrics = [\"accuracy\"])\n",
        "    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n",
        "                        verbose = 1, callbacks = [check_point, early_stop])\n",
        "    model = load_model(file_path)\n",
        "    return model"
      ],
      "metadata": {
        "trusted": true,
        "id": "RCxm4gfm8ekC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model6 = build_model3(lr = 1e-4,\n",
        "                      lr_d = 1e-7,\n",
        "                      units = 128,\n",
        "                      spatial_dr = 0.5,\n",
        "                      kernel_size1=4,\n",
        "                      kernel_size2=3,\n",
        "                      dense_units=32,\n",
        "                      dr=0.2,\n",
        "                      conv_size=64)"
      ],
      "metadata": {
        "trusted": true,
        "id": "kcDqnnri8ekD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model7 = build_model3(lr = 1e-3,\n",
        "                      lr_d = 1e-10,\n",
        "                      units = 128,\n",
        "                      spatial_dr = 0.5,\n",
        "                      kernel_size1=3,\n",
        "                      kernel_size2=2,\n",
        "                      dense_units=64,\n",
        "                      dr=0.3,\n",
        "                      conv_size=64)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Kb1lg8Mw8ekD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred1 = model1.predict(X_test, batch_size = 1024, verbose = 1)\n",
        "pred = pred1\n",
        "pred2 = model2.predict(X_test, batch_size = 1024, verbose = 1)\n",
        "pred += pred2\n",
        "pred3 = model3.predict(X_test, batch_size = 1024, verbose = 1)\n",
        "pred += pred3\n",
        "pred4 = model4.predict(X_test, batch_size = 1024, verbose = 1)\n",
        "pred += pred4\n",
        "pred5 = model5.predict(X_test, batch_size = 1024, verbose = 1)\n",
        "pred += pred5\n",
        "pred6 = model6.predict(X_test, batch_size = 1024, verbose = 1)\n",
        "pred += pred6\n",
        "pred7 = model7.predict(X_test, batch_size = 1024, verbose = 1)\n",
        "pred += pred7"
      ],
      "metadata": {
        "trusted": true,
        "id": "V-bDnVVM8ekD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = np.round(np.argmax(pred, axis=1)).astype(int)\n",
        "sub['label'] = predictions\n",
        "sub.to_csv(\"Submission_test.csv\", index=False)"
      ],
      "metadata": {
        "id": "Z_ckyA7K8ekE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using a pre-trained model AKA Roberta"
      ],
      "metadata": {
        "id": "iQ3SZ-nE8ekE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_nm = \"microsoft/deberta-v3-small\""
      ],
      "metadata": {
        "trusted": true,
        "id": "0AcVvMMA8ekE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification , AutoTokenizer\n",
        "Tokenizer = AutoTokenizer.from_pretrained(model_nm, use_fast=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "2o4fiyoc8ekF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tok_func(x): return Tokenizer(x[\"text\"],truncation=True,max_length=512)\n",
        "from datasets import Dataset, DatasetDict\n",
        "def create_dataset(df):\n",
        "    ds = Dataset.from_pandas(train_df)\n",
        "    if 'label' in train_df.columns:\n",
        "        ds = ds.rename_columns({'label': 'labels'})\n",
        "    ds = ds.map(tok_func, batched=True)\n",
        "    return ds\n",
        "Tokenizer_ds = create_dataset(train_df)"
      ],
      "metadata": {
        "trusted": true,
        "id": "NWMFVKnn8ekF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = test_df.rename(columns={\"text\":\"input\"})\n",
        "test_df"
      ],
      "metadata": {
        "trusted": true,
        "id": "rWnKFfpn8ekF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_df = valid_df.rename(columns={\"text\":\"input\"})\n",
        "valid_df[\"label\"] = valid_df.label.astype(float)\n",
        "valid_df"
      ],
      "metadata": {
        "trusted": true,
        "id": "jTGX_GKm8ekI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_ds = create_dataset(valid_df)\n",
        "test_ds = create_dataset(test_df)"
      ],
      "metadata": {
        "trusted": true,
        "id": "xwxRzDCH8ekI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 32\n",
        "epochs = 5\n",
        "lr = 8e-5 \n",
        "\n",
        "def acc(preds): \n",
        "    predictions, labels = preds\n",
        "    predictions = (predictions > 0.5).astype(int)\n",
        "    return {'accuracy': accuracy_score(labels, predictions)}\n",
        "\n",
        "args = TrainingArguments(\"outputs\",\n",
        "                         learning_rate=lr,\n",
        "                         warmup_ratio=0.1,\n",
        "                         lr_scheduler_type=\"cosine\",\n",
        "                         fp16=True,\n",
        "                         evaluation_strategy=\"epoch\",\n",
        "                         per_device_train_batch_size=bs,\n",
        "                         per_device_eval_batch_size=bs*2,\n",
        "                         num_train_epochs=epochs,\n",
        "                         weight_decay=0.01,\n",
        "                         report_to=\"none\",\n",
        "                         save_strategy='epoch',\n",
        "                         load_best_model_at_end=True,)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_nm,num_labels=1)\n",
        "trainer = Trainer(model,\n",
        "                  args,\n",
        "                  train_dataset=Tokenizer_ds,\n",
        "                  eval_dataset=valid_ds,\n",
        "                  tokenizer=Tokenizer,\n",
        "                  compute_metrics=acc)"
      ],
      "metadata": {
        "trusted": true,
        "id": "fd6S2CeR8ekJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "trusted": true,
        "id": "aod1Dpb68ekJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/kaggle/working/Pretrained_model\")\n",
        "tokz.save_pretrained(\"/kaggle/working/Pretrained_model\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "qoqilBlD8ekK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "trusted": true,
        "id": "r9balIJo8ekK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = trainer.predict(test_ds).predictions\n",
        "preds"
      ],
      "metadata": {
        "trusted": true,
        "id": "b9MqRRIm8ekK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = (preds > 0.5).squeeze().astype(int)\n",
        "submission = datasets.Dataset.from_dict({\n",
        "    'id': test_df.index,\n",
        "    'label': preds\n",
        "})\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "filiGmi58ekL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}